import os
import json
import glob
from os.path import join as pjoin
import pandas as pd

# TODO: Need to validate the inputs....
# TODO: divide rules into different file to have the main file cleaner 

# Define wildcards for chromosomes 
wildcard_constraints:
      chrom=r"\d+"

#----------------------------------------
# Read configuration file
#----------------------------------------
configfile: "config/config.yaml"

# Load genetic data information
genotype = config["genodata"]["name"]
with open(config["genodata"]["json"], "r") as f:
  tmp = json.load(f)
gd = tmp[genotype]

plinkpath = {p for p in gd["plinkfiles"]}.pop()

# Store config variables for ease access
resdir = config["sumstat"]["resdir"]
pvalcol = config["sumstat"]["pvalcol"]
pthr = config["sumstat"]["pthr"]
outdir = config["output_dir"]

if not os.path.exists(outdir):
  os.makedirs(outdir, exist_ok=True)

# Include script to prepare input
# TODO: generalize prepare_input.py script
include: "scripts/prepare_input.py"

# Check if the input is available
try:
  run_list = pd.read_csv(config["run_list"], header=0, sep="\t")
except KeyError:
  run_list = prepare_inputs(pjoin(resdir, "tophits"), pval_thr=pthr)
except FileNotFoundError:
  run_list = prepare_inputs(pjoin(resdir, "tophits"), pval_thr=pthr)


#----------------------------------------
# Custom funtion to define input for the rules
#----------------------------------------
def get_pfile_from_chrom(wildcards):
  pfiles = gd['plinkfiles']
  nfiles = gd['nfiles']
  if nfiles == 1:
    ff = pfiles
  else:
    ff = [p for p in pfiles if p.find(f'chr{wildcards.chrom}.') > 0]
  return ff[0]

# Function to get the phenotypes
def get_phenos():
  return run_list['pheno'].unique().tolist()

def get_pheno_to_run():
  fnames = glob.glob(pjoin(resdir, "*.regenie.gz.tbi"))
  phenos = [os.path.basename(f) for f in fnames]
  phenos = [p.replace(".regenie.gz.tbi", "") for p in phenos]
  return phenos
  
#----------------------------------------
# Start creating the rules for the workflow
#----------------------------------------
# Target rule
rule all:
  input:
    pjoin(outdir, "all_phenos_summary.cs")

# rule test_rule:
#   input:
#     expand(pjoin(outdir, "{pheno}/summary_credible_sets.csv"), pheno=["meta_6"])

# Rule to export bed files for each credible set
rule beds:
  input:
    expand(pjoin(outdir, "{pheno}/all_cs.bed"), pheno=get_phenos()),


# Divide sumstat by chromosome
rule sumstat_2_plink:
  message:
    "Separate summary stat by chromosome based on best hits"
  input:
    sumstat = pjoin(resdir, "{pheno}.regenie.gz")
  output:
    temp(pjoin(outdir, "{pheno}/tmp/chr{chrom}_sumstat.csv"))
  resources:
    mem_mb=12000
  params:
    pval_thr = pthr,
    pval_col = pvalcol
  conda:
    "envs/plink-pandas.yml"
  script:
    "scripts/separate_gwas.py"


rule cut_pheno:
  message:
    "Extract the phenotype from the original GWAS"
  input:
    phenofile = config["pheno_file"]
  output:
    temp(pjoin(outdir, "{pheno}/tmp/phenotype.csv"))
  conda:
    "envs/plink-pandas.yml"
  script:
    "scripts/separate_pheno.py"

rule clumping:
  message:
    "Run clumping"
  input:
    smstat = pjoin(outdir, "{pheno}/tmp/chr{chrom}_sumstat.csv")
  output:
    pjoin(outdir, "{pheno}/clumping/chr{chrom}.clumps"),
    pjoin(outdir, "{pheno}/clumping/chr{chrom}.log")
  params:
    infile = get_pfile_from_chrom,
    ofile = lambda wildcards, output : output[0].replace(".clumps", ""),
    clump_logp1 = config["clumping"]["logp1"],
    clump_logp2 = config["clumping"]["logp2"],
    clump_r2 = config["clumping"]["r2"],
    clump_kb = config["clumping"]["kb"],
    sampfile = config["sample_file"]
  resources:
    mem_mb = 16000
  conda:
    "envs/plink2.yml"
  shell:
    """
    if [ -s {input} ]
    then
      plink2 --bfile {params.infile} --clump {input.smstat} \
      --clump-log10 'input-only' --clump-field {pvalcol} \
      --clump-log10-p1 {params.clump_logp1} --clump-log10-p2 {params.clump_logp2} \
      --clump-r2 {params.clump_r2} --clump-kb {params.clump_kb} \
      --clump-snp-field ID  --out {params.ofile} \
      --memory {resources.mem_mb} \
      --keep {params.sampfile}
    else
      touch {output[0]}
      touch {output[1]}
    fi
    """

rule credible_set:
  input:
    smstat = pjoin(outdir, "{pheno}/tmp/chr{chrom}_sumstat.csv"),
  output:
    credible_set = pjoin(outdir, "{pheno}/cred_set_chr{chrom}.csv"),
    # tophits = pjoin(outdir, "{pheno}/tophist_chr{chrom}.csv")
  params:
    plinkfile = get_pfile_from_chrom,
    tmpdir = pjoin(outdir, "{pheno}/tmp")
  resources:
    mem_mb=16000
  conda:
    "envs/plink-pandas.yml"
  script:
    "scripts/run_susieR.py"


rule enlarge_and_merge:
  message:
    "Merge overlapping clumps after enlarging to at least 1Mb"
  input:
    rules.clumping.output[0],
  output:
    pjoin(outdir, "{pheno}/ld/chr{chrom}.ld")
  params:
    plinkfile = get_pfile_from_chrom,
    totsize = config["clumping"]["totsize"]
  conda:
    "envs/plink-pandas.yml"
  resources:
    mem_mb=16000
  script:
    "scripts/run_susieR.py"

rule run_susieR:
  input:
    ldfile = rules.enlarge_and_merge.output,
    smstat = pjoin(outdir, "{pheno}/tmp/chr{chrom}_sumstat.csv"),
    phenofile = rules.cut_pheno.output
  output:
    cs_smstat = pjoin(outdir, "{pheno}/cs/cs_chr{chrom}.cssmstat"),
    cs_report = pjoin(outdir, "{pheno}/cs/cs_chr{chrom}.csreport"),
    cs_rds = pjoin(outdir, "{pheno}/cs/cs_chr{chrom}_fit.rds")#,
    # cs_log = pjoin(outdir, "{pheno}/cs/cs_chr{chrom}.log")
  params:
    use_ld = config["susieR"]["use_ld"],
    chris_id = config["susieR"]["chris_id"],
    iter = config["susieR"]["iter"],
    min_abs_corr = config["susieR"]["min_abs_corr"]
  resources:
    mem_mb = 16000
  conda:
    "envs/susier.yml"
  script:
    "scripts/finemapping.R"

rule collect_by_pheno:
  input:
    expand(pjoin(outdir, "{{pheno}}/cs/cs_chr{chrom}.cssmstat"), 
           chrom=lookup(query="pheno == '{pheno}'", within=run_list, cols="chrom"))
  output:
    pjoin(outdir, "{pheno}/summary.cs")
  resources:
    mem_mb=12000
  params:
    bypheno = False,
    bed = False
  script:
    "scripts/aggregate_by_pheno.py"

rule collect_credible_sets:
  input:
    expand(pjoin(outdir, "{{pheno}}/cred_set_chr{chrom}.csv"), 
           chrom=lookup(query="pheno == '{pheno}'", within=run_list, cols="chrom"))
  output:
    pjoin(outdir, "{pheno}/summary_credible_sets.csv")
  resources:
    mem_mb=12000
  params:
    bypheno = False,
    bed = False
  script:
    "scripts/aggregate_by_pheno.py"

rule create_bed:
  input:
    expand(pjoin(outdir, "{{pheno}}/cs/cs_chr{chrom}.cssmstat"), 
           chrom=lookup(query="pheno == '{pheno}'", within=run_list, cols="chrom"))
  output:
    pjoin(outdir, "{pheno}/all_cs.bed")
  params:
    bypheno = False,
    bed = True
  script:
    "scripts/aggregate_by_pheno.py"
    
rule annotate_by_pheno:
  input:
    rules.collect_credible_sets.output
  output:
    pjoin(outdir, "{pheno}/summary_credible_annot.csv")
  params:
    tophitsdir = config["sumstat"]["tophits_dir"]
  resources:
    mem_mb = 8000
  script:
    "scripts/annotate_cs.py"

rule collect_all:
  input:
    branch(config["sumstat"]["annotate"] == True,
      then=expand(pjoin(outdir, "{pheno}/summary_credible_annot.csv"), pheno=get_phenos()),
      otherwise=expand(pjoin(outdir, "{pheno}/summary_credible_sets.csv"), pheno=get_phenos())
    )
  output:
    pjoin(outdir, "all_phenos_summary.cs")
  resources:
    mem_mb=12000
  params:
    bypheno = True,
    bed = False
  conda:
    "envs/plink-pandas.yml"
  script:
    "scripts/aggregate_by_pheno.py" 

